import json
import numpy as np
# å‡è®¾è¿™æ˜¯æˆ‘ä»¬ä¸Šä¸€è½®å®šä¹‰çš„è®¡ç®—æ ¸å¿ƒ (Calculator)
# åŒ…å« evaluate_proposal, utility_G, utility_D, utility_V ç­‰å‡½æ•°
import LLM_optimization as math_core
from typing import List, Dict, Any, Optional
from llm_model import llm
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from data_model import AgentAction, ActionType
from prompt import WORLD_CONTEXT, AGENT_DEV_PROMPT, AGENT_GOV_PROMPT, AGENT_VILLAGE_PROMPT, prompt_template, parser
from langchain_core.output_parsers import PydanticOutputParser
from color_print import gov_print, dev_print, village_print

printer = {'Government': gov_print, 'Developer': dev_print, 'Village': village_print}


def extract_speech(llm_response_json: str) -> str:
    """
    ä» LLM è¾“å‡ºçš„ JSON å­—ç¬¦ä¸²ä¸­è§£æ 'public_speech'ã€‚

    Args:
        llm_response_json: LLM è¿”å›çš„åŸå§‹ JSON å­—ç¬¦ä¸²ã€‚

    Returns:
        å…¬å¼€å–Šè¯çš„å­—ç¬¦ä¸²ã€‚å¦‚æœå¤±è´¥åˆ™è¿”å›ä¸€ä¸ªé”™è¯¯ä¿¡æ¯ã€‚
    """
    try:
        data: Dict[str, Any] = json.loads(llm_response_json)
        speech = data.get('public_speech', 'é”™è¯¯ï¼šæ‰¾ä¸åˆ° "public_speech" å­—æ®µã€‚')
        return str(speech)

    except json.JSONDecodeError:
        print(f"è§£æé”™è¯¯: æ— æ³•è§£æ LLM å“åº” '{llm_response_json}'")
        return "è§£æ LLM å“åº”å¤±è´¥ã€‚"
    except Exception as e:
        return f"æå–å–Šè¯æ—¶å‡ºé”™: {e}"


class RoundTableLLM:
    def __init__(self, llm, data_model_cls, max_round: int=5):
        self._llm = llm  # å¯ä»¥ç›´æ¥è·³è¿‡promptä¸­æŒ‡å®šè¾“å‡ºå­—æ®µçš„æ­¥éª¤
        self.parser = PydanticOutputParser(pydantic_object=data_model_cls)
        self.max_round = max_round  # æœ€å¤§çš„è®¨è®ºè½®æ•°
        self.current_round = 0
        self.history = []  # å…¨å±€ä¼šè®®è®°å½•
        self.history_length = 6
        self.proposals_log = []  # è®°å½•æ‰€æœ‰è¢«æå‡ºçš„åˆæ³•å‘é‡
        self.baseline_vector = [75.91, 8.35, 32.77, 0.0, 0.20]
        self.latest_vector = [75.91, 8.35, 32.77, 0.0, 0.20]
        self.last_speaker = 'ä¸»æŒäºº'

        # å®šä¹‰ Agents
        self.characters = {
            "Government": {
                "system": WORLD_CONTEXT + AGENT_GOV_PROMPT,
                "style": "ä¸¥è‚ƒã€è®²åŸåˆ™ã€å…³æ³¨å®è§‚æŒ‡æ ‡"
            },
            "Developer": {
                "system": WORLD_CONTEXT + AGENT_DEV_PROMPT,
                "style": "ç²¾æ˜ã€æ•°æ®é©±åŠ¨ã€ä¸€ç›´åœ¨ç®—è´¦"
            },
            "Village": {
                "system": WORLD_CONTEXT + AGENT_VILLAGE_PROMPT,
                "style": "åŠ¡å®ã€å¯¸åœŸå¿…äº‰ã€å…³æ³¨é•¿è¿œåˆ†çº¢"
            }
        }
        self.agents = {
            name: ReflectiveAgent(name, llm, info['system'], style=info['style']) for name, info in
            self.characters.items()
        }
        self._init_history()

    def print(self, content: str, agent_name: str = ''):
        if agent_name in printer:
            printer[agent_name](content)
        else:
            print(content)

    def _init_history(self):
        self.history = [
            (
                f"ä¸»æŒäºº: å„ä½ä»£è¡¨å¥½ï¼Œä¼šè®®å¼€å§‹ã€‚æˆ‘ä»¬ä»åŸºå‡†æ–¹æ¡ˆå‡ºå‘ï¼ŒåŸºå‡†æ–¹æ¡ˆåªæ˜¯å¤§è‡´çš„æ¯”ä¾‹ï¼Œå¹¶ä¸ä¸€å®šæ»¡è¶³æ‰€æœ‰çº¦æŸæ¡ä»¶ (x0)ï¼š\n"
                f"  AR={self.baseline_vector[0]}, AC={self.baseline_vector[1]}, AO={self.baseline_vector[2]}, "
                f"Delta={self.baseline_vector[3]}, Tau={self.baseline_vector[4]}\n"
                f"è¯·æ”¿åºœä»£è¡¨é¦–å…ˆæ ¹æ®æ­¤æ–¹æ¡ˆå‘è¡¨æ„è§ã€‚"
            )
        ]

    @property
    def chain(self):
        return self._llm | self.parser

    def _call_llm(self, agent_name: str, context_messages: dict) -> str:
        """
        è°ƒç”¨ LLM API
        """
        # todo: éœ€è¦æŠŠæœ€æ–°çš„å†³ç­–å‘é‡ä¼ å…¥åˆ°human message
        sys_msg = SystemMessage(content=self.agents[agent_name]['system'])
        speech_msg = HumanMessage(content=prompt_template.format(history=context_messages['content']))
        response = self.chain.invoke([sys_msg, speech_msg])
        self.print(f"ğŸ¤– [{agent_name}] æ­£åœ¨æ€è€ƒ...ç­”å¤:{response.public_speech}\n------------------------", agent_name)
        return response

    def round_discuss(self):
        while self.current_round < self.max_round:
            self.current_round += 1
            self.run_round()

    def run_round(self, speaker_order=["Government", "Developer", "Village"]):
        print(f'å¼€å§‹ç¬¬{self.current_round}/{self.max_round}è½®è®¨è®º')
        """è¿è¡Œä¸€è½®è°ˆåˆ¤"""
        for agent_name in speaker_order:
            self.print(f"\nğŸ¤ --- {agent_name} å‘è¨€ ---")

            history_message = prompt_template.format(history=self._format_history(),
                                                     round=self.current_round,
                                                     total_round=self.max_round)
            response = self.agents[agent_name].propose([history_message])
            if response['evaluation'] is None:
                self.print(f"{agent_name}: æœªæå‡ºæ–°çš„æœ‰æ•ˆæ–¹æ¡ˆ", agent_name)
                self.history.append(f"{agent_name}: æ¥å—å½“å‰æ–¹æ¡ˆï¼Œæœªæå‡ºæ–°çš„æœ‰æ•ˆæ–¹æ¡ˆ")
            else:
                print('å‘è¨€ç»“æœ')
                self.print(response['action_model'], agent_name)
                self.print(response['evaluation'], agent_name)
                speech_content = response['action_model'].public_speech
                speech_content += f"\n[ç³»ç»Ÿå…¬è¯æ•°æ®]: æ–¹æ¡ˆå¯è¡Œæ€§={response['evaluation']['status']}, WSWM={response['evaluation']['utilities']['WSWM']}, U_G={response['evaluation']['utilities']['U_G']}, U_D={response['evaluation']['utilities']['U_D']}, U_V={response['evaluation']['utilities']['U_V']}"
                self.latest_vector = response['action_model'].new_proposal_vector
                self.proposals_log.append(response)
                self.history.append(f"ç¬¬{self.current_round}è½®{agent_name}çš„å‘è¨€: {speech_content}")
                self.last_speaker = agent_name

    def _format_history(self):
        current_proposal = f'{self.last_speaker}æå‡ºçš„æœ€æ–°å†³ç­–å‘é‡ä¸º{self.latest_vector}'
        return "\n".join(self.history[-self.history_length:] + [current_proposal])  # å¢åŠ å½“å‰æ–¹æ¡ˆ


class RoundTableTooledLLM(RoundTableLLM):
    def __init__(self, llm, data_model_cls):
        super().__init__(llm, data_model_cls)
        self._llm = self._llm.with_structured_output(data_model_cls)

    @property
    def chain(self):
        return self._llm


class ReflectiveAgent:
    """
    ä¸€ä¸ªå°è£…äº†â€œæè®®-è¯„ä¼°-ä¿®æ­£â€å¾ªç¯çš„åšå¼ˆ Agentã€‚
    """

    def __init__(self, name, llm, system_prompt, style: str = '', max_retry: int = 3):
        """

        :param name:
        :param llm: langchain ChatOpenAI llm
        :param system_prompt:
        :param style: äººè®¾é£æ ¼
        """
        self.name = name
        self.max_retry = max_retry
        self.system_prompt = SystemMessage(content=system_prompt)
        self.style = style
        self.proposal_chain = llm | PydanticOutputParser(pydantic_object=AgentAction)
        print(f'{self.name} Agentåˆå§‹åŒ–å®Œæ¯•')

    def propose(self, history_proposal: List[BaseMessage]):
        """

        :param history_proposal: BaseMessage List å¯ç›´æ¥ä½œä¸ºinvokeçš„è¾“å…¥
        :return: {"action_model": ActionModel, "evaluation": math_core.evaluate_proposal}
        """
        default_messages = [self.system_prompt] + history_proposal
        eval_failure = None  # {'propose_vector': [], 'info': dict}

        for i in range(self.max_retry):
            if eval_failure is None:
                messages = default_messages
            else:
                correction_prompt = (
                        f"[ç³»ç»Ÿä¿®æ­£è¯·æ±‚]\n"
                        f"ä½ åˆšæ‰çš„ææ¡ˆ {eval_failure['propose_vector']} å› è¿åä»¥ä¸‹çº¦æŸè€Œè¢«æ‹’ç»ï¼š\n" +
                        "\n".join(f"- {v}" for v in eval_failure['info']['violations']) +
                        "\nè¯·ä½ å¿…é¡»ä¿®æ­£ä¸Šè¿°æ‰€æœ‰é”™è¯¯ï¼Œé‡æ–°æ€è€ƒå¹¶æå‡ºä¸€ä¸ªåˆè§„çš„æ–°æ–¹æ¡ˆã€‚"
                )
                messages = default_messages + [HumanMessage(content=correction_prompt)]

            printer[self.name](messages)
            action_model: AgentAction = self.proposal_chain.invoke(messages)

            if action_model.action == ActionType.PROPOSE_NEW:
                # éªŒè¯æ–°ææ¡ˆ
                eval_result = math_core.evaluate_proposal(*action_model.new_proposal_vector)
                if eval_result['status'] == 'ACCEPTED':
                    return {
                        "action_model": action_model,
                        "evaluation": eval_result
                    }
                else:
                    eval_failure = {'propose_vector': action_model.new_proposal_vector, 'info': eval_result}
            elif action_model.action == ActionType.ACCEPT:
                print(f"ğŸŸ¢ {self.name} æ¥å—å½“å‰æ–¹æ¡ˆ")
                return {
                    "action_model": action_model,
                    "evaluation": None  # æ²¡æœ‰æ–°æ–¹æ¡ˆï¼Œæ— éœ€è¯„ä¼°
                }
        # å°è¯•æ¬¡æ•°ç”¨å®Œï¼Œæ¥å—å½“å‰æ–¹æ¡ˆ
        return {
            "action_model": action_model,
            "evaluation": None  # æ²¡æœ‰æ–°æ–¹æ¡ˆï¼Œæ— éœ€è¯„ä¼°
        }


if __name__ == "__main__":
    # --- æ¨¡æ‹Ÿè¿è¡Œ ---
    table = RoundTableLLM(llm, data_model_cls=AgentAction, max_round=10)
    table.round_discuss()

    print(f'æœ€ç»ˆæ–¹æ¡ˆ: {table.latest_vector}')
